{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03fa9d14-79c8-422a-881e-7001c28dcf16",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning - Exercise 11\n",
    "Goal of the excercise is to learn how to use basic deep learning models in Scikit-learn and Keras.\n",
    "\n",
    "![meme03](https://github.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/blob/master/images/fml_11_meme_03.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4211d1a-a7c9-4908-8436-a176e031cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d309f2a-52ca-4c03-b09e-28fc029a4750",
   "metadata": {},
   "source": [
    "# ðŸ“Š Deep learning introduction\n",
    "* What types of neural networks (ANN) do you know?\n",
    "    * For what types of data are these types the most useful? Why?\n",
    "\n",
    "* What is the Multi-layer Perceptron (MLP)?\n",
    "    * ðŸ”Ž What classic ML model is the closest to MLP by its function?\n",
    "    * What is the difference between them?\n",
    "\n",
    "\n",
    "## ðŸ“Œ How does the ANN work in general?\n",
    "* What does these terms mean?\n",
    "    * Neuron\n",
    "    * Input and weight\n",
    "    * Activation function\n",
    "        * Can ANN work without it?\n",
    "    * Loss function\n",
    "    * Optimizer\n",
    "\n",
    "## Training and inference phase\n",
    "* ðŸ”Ž How does the training phase work?\n",
    "* And what about inference phase?\n",
    "    * Which one is more computionaly intesive?\n",
    "\n",
    "## ðŸ“’ Let's create an example of simple ANN using math formulas on blackboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ff8ad-7ea7-4725-aea9-edc26ca9158c",
   "metadata": {},
   "source": [
    "![img01](https://github.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/blob/master/images/fml_11_ann_01.png?raw=true)\n",
    "\n",
    "# ðŸ”Ž What is the difference between MLP and Deep learning (DL)?\n",
    "\n",
    "![img02](https://github.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/blob/master/images/fml_11_ann_02.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dacf65-820e-480d-9f1b-d3fefd7675ad",
   "metadata": {},
   "source": [
    "# ðŸš€ Enough of theory, let's try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3ee03-404a-47f8-8229-69046a49a96c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Diabetes dataset\n",
    "https://www.kaggle.com/datasets/saurabh00007/diabetescsv/data\n",
    "\n",
    "### ðŸŽ¯ Our goal is to build model able to classify if person has diabetes or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ed3d7-33ba-49b9-bfc3-67d86d78da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/master/datasets/diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43395533-6bcc-4954-ac96-ab76437c61b2",
   "metadata": {},
   "source": [
    "## Is each column numerical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afd6ab-c90a-4bd6-a6c5-8b83aef6f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925352fd-ca15-42fa-852d-a7b65d19842f",
   "metadata": {},
   "source": [
    "## Do we have any missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f76bf-ff92-472e-a15c-d15c5d09f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43e405-b8f8-4230-8035-d7b9812b6446",
   "metadata": {},
   "source": [
    "## Is class distribution balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2c449-ff56-4396-bf10-ee8e99770dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outcome.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d550696-8f40-4ef0-a4b3-ede0b194fe34",
   "metadata": {},
   "source": [
    "# âš¡ What features are more important than others?\n",
    "* Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effffd73-ec1b-4e51-ab53-7689efcb3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, df.shape[1]-1, figsize=(12, 6))\n",
    "\n",
    "for i, col in enumerate(df.columns[:-1]):\n",
    "    ax = axes.flatten()[i]\n",
    "    sns.boxplot(data=df, y=col, x='Outcome', ax=ax, hue='Outcome')\n",
    "        \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc14ea-e2bb-434f-8356-aaaa51af0473",
   "metadata": {},
   "source": [
    "## Now we can split the data into train/test set and proceed as usuall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eebe1d-c8d5-45e4-b5b8-c599b6454a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('Outcome', axis=1), df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b548b7-68e1-4560-b185-0ffdfb15f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c320c-410e-4c7b-aca5-64a8f659b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621cf01-6846-4751-a047-ea76e4042d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e39573-01bf-4ceb-b2db-90256aeed73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4153837-af5d-43ec-b3b7-2c31964deb10",
   "metadata": {},
   "source": [
    "# ðŸš€ We will use the `MLPClassifier` model\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "\n",
    "* ðŸ’¡ The sklearn ANN API is very basic, there are only a few parameters that are worth tuning\n",
    "    * hidden_layer_sizes - the ANN hidden layers structure (tuple)\n",
    "    * activation - activation function used in hidden layers\n",
    "    * solver - optimizer\n",
    "    * max_iter - number of epochs\n",
    "    * batch_size - number of instances in one batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c6121-6132-4db6-ab87-9f878ff349d1",
   "metadata": {},
   "source": [
    "## The pipeline is the same as with any other model\n",
    "* We will define the model\n",
    "* Fit it on training data\n",
    "* Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8d2bd-895b-4ec2-9893-3b6b9b02e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(8,), activation='relu', solver='adam', max_iter=300, batch_size=32, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53908ad-f501-47ba-a5b3-1d48e930a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941650b-182d-4805-a2ee-e27ebbf42a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7c8a7-4b24-428a-8946-0e312d7d0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a956dc6f-33c9-4e54-af68-abeeb9495396",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "ConfusionMatrixDisplay(cm, display_labels=clf.classes_).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84127be-a71f-4b03-80dd-a08245372cfc",
   "metadata": {},
   "source": [
    "# ðŸ’¡ You can see that it does not matter if you are using ML or DL models\n",
    "## The key principles are still the same ðŸ™‚\n",
    "\n",
    "![meme01](https://github.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/blob/master/images/fml_11_meme_01.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56c453-7e66-480c-b1bf-abd123f3b584",
   "metadata": {},
   "source": [
    "# ðŸ”Ž Can we improve the performace somehow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb8511-d0fa-48ea-a1cc-f679a78ae639",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(64,32), activation='relu', solver='adam', max_iter=300, batch_size=32, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c92bf13-36ed-45a5-884c-e45e95acf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e18a64-5b45-4024-9342-fdeeb4fccd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432f3cf-d450-4e3b-98bb-afba628eb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e4eb2-83f9-4dcc-be16-4656566c41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "ConfusionMatrixDisplay(cm, display_labels=clf.classes_).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cbc83-2beb-4f33-a204-6247d94ad03e",
   "metadata": {},
   "source": [
    "## âš¡ We've come to a conclusion that ANN and Linear regression is pretty similar in principle earlier\n",
    "* Do you remember what is a general issue for optimization algorithms?\n",
    "    * ðŸ’¡ It is about a differences in the input features\n",
    " \n",
    "* The model can be made much simpler with a preprocessing employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b352e2-691d-4750-bdf5-8553d0532a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train)\n",
    "X_train_std = std_scaler.transform(X_train)\n",
    "X_test_std = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e0be0-f9cc-4417-b34e-66aedfd5c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(8,), activation='relu', solver='adam', max_iter=100, batch_size=32, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc738435-d923-4a32-aa66-2df07b779cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0293c0-a6fb-49a6-903b-c5652bbe6597",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfcbf5-2bc0-4a76-827f-2252c6c38fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a831f-c36c-4806-878e-903207c62fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "ConfusionMatrixDisplay(cm, display_labels=clf.classes_).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511155f0-8dc4-49a4-a1f1-02f01ef32a2b",
   "metadata": {},
   "source": [
    "# The Scikit-learn has not much more to offer unfortunately...\n",
    "\n",
    "![meme02](https://github.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/blob/master/images/fml_11_meme_02.jpg?raw=true)\n",
    "\n",
    "# ðŸš€ ... and that's the the reason why there are other frameworks focused on ANN\n",
    "* Do you know any?\n",
    "    * Have you worked with some of them already?\n",
    " \n",
    "## âš¡ We will learn the basics of the Tensorflow/Keras using a minimal example on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90f520-c2db-4b63-a8e0-58cf663e5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383e59e-a109-490b-81a5-e109ad32df67",
   "metadata": {},
   "source": [
    "# ðŸ’¡ MNIST is the basic dataset with handwritten digits\n",
    "* The data are in the form of 28x28 pixes with values 0-255\n",
    "* The dataset consists of 60,000 training images and 10,000 testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb7e8b-0fc9-40c9-9d7a-013e853d2bdd",
   "metadata": {
    "id": "OK0bAkspjdMI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Test data shape:  ', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998d146-6978-4390-b1c1-9633a607c0df",
   "metadata": {
    "id": "ACxVYmJGjdMI"
   },
   "source": [
    "# Let's look on the data\n",
    "* ðŸ”Ž Why are the values in 0-255 range? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bddd1-869a-4f2b-9986-1cff8eeb54c8",
   "metadata": {
    "id": "OZaKJJf3jdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[1])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8961d-6f10-419c-aa4f-02c58972e704",
   "metadata": {},
   "source": [
    "# We can see that the numbers are already centered\n",
    "* You won't see this feature in the real world images often ðŸ™‚\n",
    "* Some of the images are quite easy to classify but on the other hand there is a lot of noise in the data as well.\n",
    "\n",
    "## ðŸ”Ž What number is on image below? Is it 1 or 7? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1105496-c396-4ff8-aea5-09b0a336c436",
   "metadata": {
    "id": "OZaKJJf3jdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[42])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576df32e-f720-4344-94c9-99cb7633f845",
   "metadata": {
    "id": "7Z_rfPvrjdMJ"
   },
   "source": [
    "# Lets normalize the values into the range \\(0,1\\) by dividing it 255.\n",
    "* I guess that you already know why we do this preprocessing step ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af97f00-6ace-41a0-bf2c-ec4ae12399cd",
   "metadata": {
    "id": "hCNrX5rqjdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760ea63-cdf9-4eb2-8445-56f819ef3d93",
   "metadata": {
    "id": "tyhe6lUKjdMJ"
   },
   "source": [
    "# ðŸ“ˆ It's worth to make better visualization of the data to understand how complex they are\n",
    "* You can see that \"noisy\" digits and different handwriting styles are common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a7252-1447-4c8b-bd2d-305f9a4f4af3",
   "metadata": {
    "id": "dIf8LNL7jdMJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = [str(x) for x in range(10)]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2ba78-b559-452a-b201-5429f560d0b2",
   "metadata": {},
   "source": [
    "# ðŸŒ³ Before we dive in the deep learning territory, let's try to create some baseline using machine learning so we can compare the approaches\n",
    "* Are ML models capable of processing image data?\n",
    "* ðŸ”Ž How to deal with a matrix input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b993af-daa8-4895-aca0-654270efd27e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alg = DecisionTreeClassifier(random_state=13)\n",
    "alg.fit(x_train.reshape(-1, 28*28), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cca751-281d-478e-95d2-67721302481b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = alg.predict(x_test.reshape(-1, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de4ec7-5c4b-4fde-8da8-97cfd487c5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6e473-d460-4ab6-8094-003384967248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c656c0-c2ce-4a9b-be2f-e3af0d44f83f",
   "metadata": {},
   "source": [
    "# We can see that we were able to create a classifier very easily\n",
    "* However it is no secret, that DL models accuracy on MNIST can be >= 98% even with simple architecture\n",
    "* Given the fact, our result is not very impresive ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860bc3cc-06c8-4fde-aae6-c207f9fbfe24",
   "metadata": {
    "id": "H3uY8cfbjdMJ"
   },
   "source": [
    "# ðŸš€ Let's design our first ANN in Keras\n",
    "* Model is created using layers, many layers exists in the [layer submodule](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "* Each layer uses a activation functions collected in the [module nn](https://www.tensorflow.org/api_docs/python/tf/nn)\n",
    "* There are 2 ways of using the Keras API, **Sequential** and **Functional**\n",
    "    * We will use the Sequential one\n",
    " \n",
    "## ðŸ’¡ Notice the last (output) layer\n",
    "* Why do we have 10 neurons?\n",
    "* Why do we use `softmax` activation?\n",
    "    * Do you know any other common activation functions used in output layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e225caa-56f6-45ab-b84b-c759c0f6e846",
   "metadata": {
    "id": "4FRfWkocjdMK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),# Flatten module flatten the multidimension input into single vector 28x28 = 784 float numbers\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu), # standard dense-fully connected layer with the rectified lineaar function as an activation\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax), # another fully-connected layer with softmax activation function\n",
    "])\n",
    "\n",
    "model.summary() # prints the summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77e1d3-ba6d-45c4-b6f9-eba144163565",
   "metadata": {},
   "source": [
    "# ðŸ”Ž What is the meaning of the `Total params` number?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f54d5-acf9-4f10-a621-ef9d61647705",
   "metadata": {
    "id": "XxkeKMLdjdMK"
   },
   "source": [
    "# ðŸ“ŒEach model need to be compiled to be able to fit to the data and predict the labels\n",
    "* There are 3 important parameters we need to set\n",
    "\n",
    "### 1 - Optimizer\n",
    "* There are many optimizers available, you can experiment with different algorithms, most of them are based on gradient descent algorithm\n",
    "\n",
    "### 2 - Loss function\n",
    "* Training of the ANN is about weights optimization\n",
    "* We need to some formula that says us if the optimization process is making the ANN better or not\n",
    "\n",
    "* **The choice of a loss function depends on tha task and network architecture. Below are the most common loss functions mentioned.**\n",
    "    * Mean Squared Error\n",
    "       * A classical measure to be used in regression\n",
    "    * Binary Cross-Entropy\n",
    "       * Predict the class from the set {0,1}\n",
    "       * Requires a sigmoid activation function\n",
    "    * Categorical Cross-Entropy\n",
    "       * Default for mutli-class classification problems\n",
    "       * Requires the softmax function on output layer to compute probability of each label\n",
    "       * Labels have to be one-hot-encoded\n",
    "    * Sparse Categorical Cross-Entropy\n",
    "       * The same as above, but the lables are not encoded\n",
    "\n",
    "### 3 - Metric\n",
    "* Used metrics for the model prediction accuracy evaluation are the same as in the ML area, i.e. accuracy, f1-score, recall, etc.\n",
    "* The choice depends on the task and the labels distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081157c4-7145-41a5-b574-d78d6bd96262",
   "metadata": {},
   "source": [
    "# âš¡ We can compile the model now\n",
    "* We will use the accuracy metric\n",
    "* Why do we use `SparseCategoricalCrossentropy`?\n",
    "    * What is `from_logits` parameter?\n",
    "\n",
    "* `SparseCategoricalCrossentropy` -> We expect labels to be provided as integers\n",
    "\n",
    "* The output of the Dense layer will either return:\n",
    "    * **probabilities**: The output is passed through a SoftMax function which normalizes the output into a set of probabilities over N neurons, that all add up to 1\n",
    "    * **logits**: Raw output of N activations - no normalization by SoftMax applied\n",
    "\n",
    "* Your loss function has to be informed as to whether it should expect a normalized distribution (output passed through a SoftMax function) or logits\n",
    "    * ðŸ’¡ If your output layer has a 'softmax' activation, `from_logits` should be `False`\n",
    "    * ðŸ’¡ If your output layer doesn't have a 'softmax' activation, `from_logits` should be `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca165b-b7e7-4638-b37f-5ee86a7967ff",
   "metadata": {
    "id": "1rh03ZvLjdMK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics = ['accuracy'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228f192-ad45-4d7c-824e-16544942a943",
   "metadata": {
    "id": "lA09suMrjdMK"
   },
   "source": [
    "# Model visualization\n",
    "* The model may be printed into image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b77d56-ee90-4f0b-9686-0e37e137879a",
   "metadata": {
    "id": "RAKg-e-OjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2babec2-087e-4f3a-b56f-842b3d34f897",
   "metadata": {
    "id": "An4hyfD7jdML"
   },
   "source": [
    "# âš¡ Now we can fit the model to the input data\n",
    "* The `fit()` method fit the model to the data\n",
    "    * The parameters are *data* and *labels* from the train set and number of *epoch* to be trained\n",
    "* The `validation_split` parameter is also very common\n",
    "    * What does the parameter do?\n",
    "    * ðŸ”Ž What is the validation set?\n",
    "    * How is it different from the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251dcde6-ee39-4ad3-8784-f9363b17479c",
   "metadata": {},
   "source": [
    "# Define the callbacks\n",
    "* ðŸ’¡ Always use ModelCheckpoint callback so you overcome the possible overfitting in the last few epochs!\n",
    "- **The best weights are determined using the validation loss value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0075d20-e5ab-4b67-948a-ffdb163ee53c",
   "metadata": {
    "id": "NUzeNn2JjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights.best.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4f794-c924-4428-9a11-dfae28c79614",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b999d77-a1d1-46dc-afe0-47f5a04747e0",
   "metadata": {
    "id": "ixodDENSjdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.2, epochs=15, callbacks=[model_checkpoint_callback], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f81c5-7d9e-45c7-88dd-19434d4fd3f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Let's take a look at the accuracy and loss function values of train and validation set\n",
    "* What can you see in the plot?\n",
    "* Is OK that loss is getting lower and accuracy higher?\n",
    "    * ðŸ”Ž Can the loss function value go higher? \n",
    "* What if the training set loss is getting lower, but validation set loss higher? \n",
    "    * ðŸ”Ž How do we call this situation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ac3a8-221b-46f7-934e-aa3c1f76742e",
   "metadata": {
    "id": "Wj3duiFljdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for key in history.history.keys():\n",
    "    plt.plot(history.epoch, history.history[key], label=key)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d0d72f-ae7f-4b8e-9d3b-52fcca4e30ce",
   "metadata": {
    "id": "O3hqCxJ8jdML"
   },
   "source": [
    "# ðŸ’¡ Beware that the best weights needs to be loaded after the training is finished!\n",
    "* **Otherwise you use the weights from the last epoch!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa9e14-9faa-4ff5-b894-b8424f4618ec",
   "metadata": {
    "id": "rkuf66s5jdML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d751b2f-7c7c-430b-b2fa-b382f4a87385",
   "metadata": {},
   "source": [
    "# We can use `evaluate()` function for obtaining the accuracy using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd0ba8-35c5-4bcc-b6cc-69e7d205c909",
   "metadata": {
    "id": "x0QAkwi7jdMM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd0667-f861-4bcb-af6b-0c0235c4a045",
   "metadata": {},
   "source": [
    "# ðŸ“’ Can we obtain the labels and use it for our own evaluation without Keras? Sure!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e261d5be-a65c-41b2-8f4c-40ef0db64c32",
   "metadata": {},
   "source": [
    "## We will obtain the raw softmax outputs first\n",
    "* What is the range of the vector values?\n",
    "* Do they sum-up to some number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc062b0-731d-44b5-8ca2-eb45f6ff8e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4a3c1-9ed9-4edf-bf1c-5709cab21aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_proba[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823fab3c-9e0d-41ba-b283-66cc06b8fcdc",
   "metadata": {},
   "source": [
    "## The numbers are hard to read, let's rather create barplot ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4d46f-3fa2-46f8-b079-472e27324b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=np.arange(0, len(y_pred_proba[0])), y=y_pred_proba[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55545e-d476-4fcd-8081-322ead2339db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.sum(y_pred_proba[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33792e51-0357-4d3e-b781-88ad1f522f4a",
   "metadata": {},
   "source": [
    "## We need to extract the index of the highest probability to get the label\n",
    "* ðŸ”Ž What will be the output in our case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319612ee-b7fc-46ff-aadf-e9d6f8f04e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_pred_proba[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd51848-d3fc-416d-b7e7-3b95d62b8a8f",
   "metadata": {},
   "source": [
    "## And finally we can use the `argmax` on the whole output matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51dfb99-2682-492b-98f2-8c5a341025f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8548750-a1d5-485c-8a58-4a30260f42f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2ece6-b434-4ca8-98cd-272848bef4ab",
   "metadata": {},
   "source": [
    "## ... and compute the `accuracy_score` âœŒ\n",
    "* Did we beat the ML baseline? ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc776307-1a9a-4300-ade8-9c6c68846c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_test, y_pred=y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
