{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning - Exercise 5\n",
    "* We will learn how to use another clustering algorithm - **Hierarchical (or Agglomerative) clustering**. \n",
    "* The base principles and important hyper-parameters will be explained.\n",
    "\n",
    "![meme04](https://github.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/blob/master/images/fml_05_meme_04.jpg?raw=true)\n",
    "\n",
    "<!-- ![meme03](https://github.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/blob/master/images/fml_05_meme_03.png?raw=true) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6XOUhvFa-wT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests, zipfile, io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downlaod the data and unzip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_url = r'https://github.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/raw/master/datasets/data_clustering.zip'\n",
    "r = requests.get(zip_file_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí° Lets take a look at our data files first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dict()\n",
    "for data_file in ['clusters3', 'clusters5', 'clusters5n', 'annulus', 'boxes', 'densegrid']:\n",
    "    data = np.loadtxt(f'{data_file}.csv', delimiter=';')\n",
    "    datasets[data_file] = data\n",
    "    plt.figure()\n",
    "    plt.scatter(data[:, 0], data[:, 1])\n",
    "    plt.title(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé Why do we have multiple clustering methods?\n",
    "* üîé Why isn't k-means enough?\n",
    "\n",
    "![meme01](https://github.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/blob/master/images/fml_05_meme_01.png?raw=true)\n",
    "\n",
    "# üìä Agglomerative clustering\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "- Take a look at https://stats.stackexchange.com/questions/195446/choosing-the-right-linkage-method-for-hierarchical-clustering\n",
    "\n",
    "* üí° Agglomerative clustering is a **bottom-up approach** to clustering.\n",
    "    * We first divide **each instance into its own cluster** and **merge them into couples** based on a similarity metric.\n",
    "    * Subsequently we compute the **similarity** again and **merge the couples into bigger groups** (clusters).\n",
    "    * These groups are then merged into the bigger ones **till there is only a one big group containing all the instances present at the top**.\n",
    "\n",
    "### üöÄ **We don't have to know the number of clusters beforehand.**  \n",
    "* üí° We can perform the whole clustering process and **select the appropriate number of clusters afterward** based on the obtained results. \n",
    "* We usually use the **dendrogram** for the distance threshold estimation.\n",
    "\n",
    "## üîé You may ask how is the similarity among groups computed. \n",
    "* üí° **Similarity is pretty hard to define.**\n",
    "* There are various ways how we can compute the value called **Linkage**.\n",
    "\n",
    "![meme02](https://github.com/rasvob/VSB-FEI-Fundamentals-of-Machine-Learning-Exercises/blob/master/images/fml_05_meme_02.jpg?raw=true)\n",
    "\n",
    "## üí° Linkage variants\n",
    "- **Maximum or Complete linkage**:\n",
    "    - The distance between two clusters is defined as the maximum value of all pairwise distances between the elements in cluster 1 and the elements in cluster 2.\n",
    "    - It tends to produce more compact clusters.\n",
    "    - It is less prone to outliers than Single linkage.\n",
    "    - Complete linkage methods tend to break large clusters.\n",
    "\n",
    "\n",
    "- **Minimum or Single linkage**:\n",
    "    - The distance between two clusters is defined as the minimum value of all pairwise distances between the elements in cluster 1 and the elements in cluster 2.\n",
    "    - It tends to produce long, ‚Äúloose‚Äù clusters.\n",
    "    - Single linkage method is prone to \"chain\" and form clusters of irregular, often thread-like curved shapes.\n",
    "        - The reason for that is obvious. With this method, at any step, **two clusters are merged if their closest edges are close enough**.\n",
    "        - No proximity between other parts of the two clusters is taken into consideration.\n",
    "\n",
    "\n",
    "- **Mean or Average linkage**:\n",
    "    - The distance between two clusters is defined as the average distance between the elements in cluster 1 and the elements in cluster 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to take a look at dendrogram, but we will be using only smaller portion of data *clusters3* for now\n",
    "* Use `np.random.choice` for choosing only 30 samples\n",
    "* Check the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets['clusters3']\n",
    "# data_subset = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_subset[:, 0], data_subset[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    # Credit to https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    plt.ylabel('distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Complete linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to calculate whole hiearchical clustering tree before dendrogram calculation\n",
    "\n",
    "- üí° We can achieve this by setting distance_threshold=0, n_clusters=None in sklearn.cluster.AgglomerativeClustering\n",
    "\n",
    "- Notice the height of the vertcal lines in the denrogram - the higher the line (called fusion), the lower is the similarity between the clusters.\n",
    "- This similarity is called cophenetic distance.\n",
    "\n",
    "* In scikit.learn we have the `AgglomerativeClustering` class\n",
    "    * Important parameters are *linkage, distance_threshold* and *n_clusters*\n",
    "    * And also there is the *fit()* method as usual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_dendrogram(clustering, labels=clustering.labels_, color_threshold=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé What can be an ideal level for splitting our data to clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(clustering, labels=clustering.labels_, color_threshold=11)\n",
    "plt.axhline(y=11, color='black', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Create dendrogram for the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_dendrogram(clustering, labels=clustering.labels_, color_threshold=11)\n",
    "plt.axhline(y=11, color='black', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's generate clustering results and use sccater plot for visualizion for our 2D toy datasets\n",
    "\n",
    "* We can use **one of following parameters** to obtain our clusters from AgglomerativeClustering:\n",
    "    - **distance_thrashold** - this is effectively the split line above\n",
    "        - i.e. if the distance between two clusters is higher than the *distance_thrashold*, then these clusters won't be merged\n",
    "    - **n_clusters** - similar to the k-means *k* parameter\n",
    " \n",
    "## üí° We can start with `distance_threshold` and set it to `11`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí°  We can also specify number of clusters directly\n",
    "* Set `n_clusters` to `3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Single linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(clustering, labels=clustering.labels_, color_threshold=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé What is the appropriate number of clusters?\n",
    "* Use `distance_threshold` to set it and take a look at how big are the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.Series(clustering.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé What is the difference between this and complete linkage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can use Agglomerative clustering for the annulus dataset as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets['annulus']\n",
    "plt.scatter(data[:,0], data[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Complete linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(linkage='complete', distance_threshold=0, n_clusters=None)\n",
    "clustering = clustering.fit(data)\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(clustering, labels=clustering.labels_, color_threshold=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four clusters seems OK, so let's try it ‚úä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(linkage='complete', distance_threshold=None, n_clusters=4)\n",
    "clustering = clustering.fit(data)\n",
    "pd.Series(clustering.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does it seem OK now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Single linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(linkage='single', distance_threshold=0, n_clusters=None)\n",
    "clustering = clustering.fit(data)\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(clustering, labels=clustering.labels_, color_threshold=999)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try with two clusters now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(linkage='single', distance_threshold=None, n_clusters=2)\n",
    "clustering = clustering.fit(data)\n",
    "pd.Series(clustering.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ We have seen, that there is no clear winner among linkage parameters\n",
    "- It depends solely on the dataset \n",
    "- **No Free Lunch Theorem** - see https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/\n",
    "\n",
    "## For more clustering information you can take a look at [documentation](https://scikit-learn.org/stable/modules/clustering.html#clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Task (2p)\n",
    "- Use **Agglomerative Clustering** on **clusters5** and **densegrid** datasets\n",
    "- Detect \"ideal\" number of clusters using the dendrogram\n",
    "- **Use both, the single and complete linkage and compare the results of the clustering methods**\n",
    "    - How are the selected clusters aligned with the raw data (use scatter plots)?\n",
    "    - i.e. You can guess the right number of clusters by taking a look at the data, but is the result of the algorithm right from your point of view?\n",
    " \n",
    "* **Describe the insight you got from the plots with a few sentences in a Markdown cell below the plot**\n",
    "  * ‚ùå Plot interpretation figured in real-time during task check is not allowed! ‚ùå"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cv4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
